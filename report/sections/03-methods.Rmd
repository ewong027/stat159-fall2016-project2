#Methods
```{r, echo = FALSE, include = FALSE}
load("../../data/RData-files/lasso-regression.RData")
load("../../data/RData-files/pls-regression.RData")
```
##Ordinary Least Squares Regression

##Ridge Regression 

##Lasso Regression
Lasso regression is very similar to ridge regression when it comes to the process. Howvever, in lasso regression we minimize $$\sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}{p}\beta_jx_{ij})^2+\lambda\sum_{j=1}^{p}|\beta_j|$$. This is called a shrinkage method because we are puttting a limit to the "size" of the $\beta$ coefficents. Unlike in ridge regression, this method of minimization could result in estimations of the $\beta_j$ as exaclty zero. This means Lasso regression performs a variable selection and only fits a model to a subset of the data. 
In order to calcualte the best $\lambda$ for the minimization, we used ten-fold cross validation. Cross validation involves partitioning the data into subsets, fitting a model with one subset, and validating it on the the other. We select a $\lambda$ based off of the validation with the least amount of error. When we ran our cross validation on the `scaled-credit.csv` data set, we came up with a lambda of `r cat(lambda_min_lasso)`. 

##Principal Components Regression 

##Partial Least Squares Regression
Partial least squares regression, PLSR is an extension of principal components regression. It also works at finding the best linear combinations, but takes into account both the response and predictor variables. The idea behind this is to find a set of data $Z_i$ that would create a better fit for the data than the original data would be.
These $Z_i$ are calculated using the scaled data from `scaled-data.csv`. The process of calculating these values is iterative, so we begin with the first value$Z_1$. Mathematically, the formula is $$Z_1=\sum_{j=1}^p\phi_{j1}X_j$$ where $X_j$ is the jth explanatory variable and $\phi_{j1}$ is the coefficient when you perform simple linear regression of $Y$ onto $X_j$. This process gives more wieght to values that have more of a relationship with the response variable. 
Then to calcualte $Z_2$ you regress each of the varibles on $Z_1$. The residuals from these predictions will be the new set variables, $\tilde{X_i}$ on which we will perform the same process as for the $X_i$. So we will say $$Z_2=\sum_{j=1}^p\phi_{j2}\tilde{X_j}$$ where $\phi_{j2}$ is the coefficient from simple linear regression of $Y$ onto $\tilde{X_j}$. This process in continued M times. M is chasen in the cross validation step. Then once we have $Z_1, Z_2, \dots, Z_M$ we now fit a linear model in the same way as in PCR. 