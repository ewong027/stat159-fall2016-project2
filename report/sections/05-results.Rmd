#Results

The plot `Estimated Regression Coefficients Separated by Variable and Regression` compares the coefficients of each variable. This representation allows us to compare how each type of regression fits a a model to the data. For instance we can clearly see that all but OLS regression has an intercept of exactly zero. Also, we can see that Lasso regression results in some of the coefficients being exactly zero. We can also see that some variables result in estimates that are fairly the same regardless of the type of regression like `Income` and `StudentYes`. While some variables like `Rating` and `Limit` have coefficient estimates that vary widely by different regression types. 

```{r, echo = FALSE, message = FALSE, warning=FALSE}
# creating a data frame to be able to  plot the ceofficients.
coef_df <- data.frame(type = colnames(coef_matrix), t(abs(coef_matrix)))
# changing the variable names to indicate which were affected by the absolute
# value calculation.
colnames(coef_df) <- c('type', 'Intercept', 'Abs(Income)', 'Limit', 'Rating',
                       'Cards', 'Abs(Age)', 'Abs(Education)', 
                       'Abs(GenderFemale)', 'StudentYes', 'MarriedYes', 
                       'EthnicityAsian','EthnicityCaucasian')

# tiding the data so that it is easier to plot in ggplot. 
coef_melt <- melt(coef_df, id.vars = 'type')

# changing the variable into a factor so that the x axis is not alphabatized.
coef_melt$type <- factor(coef_melt$type, levels = coef_melt$type)

# Plotting the different coefficients in barcharts. 
ggplot(coef_melt, aes(type,value))+
  geom_bar(aes(fill = type), stat = 'identity')+
  facet_wrap(~variable, scales = 'free')+
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank())+
  scale_fill_manual(values = c('#fe6f5e', '#ffcc33', '#afe313', '#95e0e8',
                               '#7070cc'))+
  ggtitle(label = 'Estimated Regression Coefficients by Variable and Regression')

```


```{r results= 'asis', echo = FALSE}
Regression <- c('OLS','Ridge', 'Lasso', 'PCR', 'PLSR')
MSE <- c(ols_MSE, ridge_MSE, lasso_mse, pc_MSE, pls_mse)
mse_chart <- data.frame(Regression, MSE)
mse_tbl <- xtable(mse_chart,
               caption = 'Test MSE Values for the Regression Techniques',
               digits = 7)

print(mse_tbl, caption.placement = 'top',
      comment = getOption("xtable.comment", FALSE),
      include.rownames = FALSE)
```

When looking at the table called Test MSE Values for the Regression Techniques, we found that all of the mean squared errors (MSE) are very close to one another with the maximum difference being `r ridge_MSE-lasso_mse`. We found of all the regressions the one with the smallest MSE is lasso regression, where MSE = `r lasso_mse`. Since lasso regression has the smallest MSE, this means that it provides the best fit model for the credit data set. On the other hand, the one with the largest MSE is Ridge Regression, where MSE = `r ridge_MSE`. This means that ridge regression provides the worst fit model of the regressions that we looked at for the Credit data set. Finally, looking at the other MSEs, we noticed that PCR and PLSR had the most similar MSEs with one another. 
