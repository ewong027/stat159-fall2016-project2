This is the output of the ten-fold cross validation using Lasso regression
 on the training data set
$lambda
  [1] 1.000000e+10 7.564633e+09 5.722368e+09 4.328761e+09 3.274549e+09
  [6] 2.477076e+09 1.873817e+09 1.417474e+09 1.072267e+09 8.111308e+08
 [11] 6.135907e+08 4.641589e+08 3.511192e+08 2.656088e+08 2.009233e+08
 [16] 1.519911e+08 1.149757e+08 8.697490e+07 6.579332e+07 4.977024e+07
 [21] 3.764936e+07 2.848036e+07 2.154435e+07 1.629751e+07 1.232847e+07
 [26] 9.326033e+06 7.054802e+06 5.336699e+06 4.037017e+06 3.053856e+06
 [31] 2.310130e+06 1.747528e+06 1.321941e+06 1.000000e+06 7.564633e+05
 [36] 5.722368e+05 4.328761e+05 3.274549e+05 2.477076e+05 1.873817e+05
 [41] 1.417474e+05 1.072267e+05 8.111308e+04 6.135907e+04 4.641589e+04
 [46] 3.511192e+04 2.656088e+04 2.009233e+04 1.519911e+04 1.149757e+04
 [51] 8.697490e+03 6.579332e+03 4.977024e+03 3.764936e+03 2.848036e+03
 [56] 2.154435e+03 1.629751e+03 1.232847e+03 9.326033e+02 7.054802e+02
 [61] 5.336699e+02 4.037017e+02 3.053856e+02 2.310130e+02 1.747528e+02
 [66] 1.321941e+02 1.000000e+02 7.564633e+01 5.722368e+01 4.328761e+01
 [71] 3.274549e+01 2.477076e+01 1.873817e+01 1.417474e+01 1.072267e+01
 [76] 8.111308e+00 6.135907e+00 4.641589e+00 3.511192e+00 2.656088e+00
 [81] 2.009233e+00 1.519911e+00 1.149757e+00 8.697490e-01 6.579332e-01
 [86] 4.977024e-01 3.764936e-01 2.848036e-01 2.154435e-01 1.629751e-01
 [91] 1.232847e-01 9.326033e-02 7.054802e-02 5.336699e-02 4.037017e-02
 [96] 3.053856e-02 2.310130e-02 1.747528e-02 1.321941e-02 1.000000e-02

$cvm
  [1] 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302
  [7] 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302
 [13] 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302
 [19] 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302
 [25] 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302
 [31] 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302
 [37] 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302
 [43] 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302
 [49] 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302
 [55] 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302
 [61] 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302
 [67] 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302
 [73] 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302
 [79] 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302 0.96295770
 [85] 0.71070822 0.51531351 0.40309817 0.33850317 0.29566081 0.25242381
 [91] 0.21933779 0.14597905 0.10403421 0.08008750 0.06622784 0.05775197
 [97] 0.05283310 0.05004592 0.04832458 0.04740189

$cvsd
  [1] 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126
  [7] 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126
 [13] 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126
 [19] 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126
 [25] 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126
 [31] 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126
 [37] 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126
 [43] 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126
 [49] 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126
 [55] 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126
 [61] 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126
 [67] 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126
 [73] 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126
 [79] 0.060279126 0.060279126 0.060279126 0.060279126 0.060279126 0.060307231
 [85] 0.046122064 0.030427010 0.024053241 0.021635268 0.020721027 0.015951116
 [91] 0.015011068 0.010232109 0.007224088 0.005290131 0.004109628 0.003423307
 [97] 0.003049356 0.002897638 0.002849742 0.002871984

$cvup
  [1] 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214
  [7] 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214
 [13] 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214
 [19] 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214
 [25] 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214
 [31] 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214
 [37] 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214
 [43] 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214
 [49] 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214
 [55] 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214
 [61] 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214
 [67] 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214
 [73] 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214
 [79] 1.02329214 1.02329214 1.02329214 1.02329214 1.02329214 1.02326493
 [85] 0.75683028 0.54574052 0.42715141 0.36013843 0.31638184 0.26837493
 [91] 0.23434886 0.15621116 0.11125830 0.08537763 0.07033747 0.06117528
 [97] 0.05588245 0.05294356 0.05117432 0.05027387

$cvlo
  [1] 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389
  [7] 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389
 [13] 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389
 [19] 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389
 [25] 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389
 [31] 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389
 [37] 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389
 [43] 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389
 [49] 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389
 [55] 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389
 [61] 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389
 [67] 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389
 [73] 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389
 [79] 0.90273389 0.90273389 0.90273389 0.90273389 0.90273389 0.90265047
 [85] 0.66458615 0.48488650 0.37904493 0.31686790 0.27493978 0.23647270
 [91] 0.20432672 0.13574695 0.09681012 0.07479737 0.06211822 0.05432867
 [97] 0.04978374 0.04714829 0.04547484 0.04452990

$nzero
 s0  s1  s2  s3  s4  s5  s6  s7  s8  s9 s10 s11 s12 s13 s14 s15 s16 s17 s18 s19 
  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
s20 s21 s22 s23 s24 s25 s26 s27 s28 s29 s30 s31 s32 s33 s34 s35 s36 s37 s38 s39 
  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
s40 s41 s42 s43 s44 s45 s46 s47 s48 s49 s50 s51 s52 s53 s54 s55 s56 s57 s58 s59 
  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
s60 s61 s62 s63 s64 s65 s66 s67 s68 s69 s70 s71 s72 s73 s74 s75 s76 s77 s78 s79 
  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 
s80 s81 s82 s83 s84 s85 s86 s87 s88 s89 s90 s91 s92 s93 s94 s95 s96 s97 s98 s99 
  0   0   0   0   2   2   2   2   3   3   4   4   4   4   5   6   6   8   8   9 

$name
                 mse 
"Mean-Squared Error" 

$glmnet.fit

Call:  glmnet(x = X, y = train_set$Balance, lambda = grid, intercept = FALSE,      standardize = FALSE, alpha = 1) 

       Df   %Dev    Lambda
  [1,]  0 0.0000 1.000e+10
  [2,]  0 0.0000 7.565e+09
  [3,]  0 0.0000 5.722e+09
  [4,]  0 0.0000 4.329e+09
  [5,]  0 0.0000 3.275e+09
  [6,]  0 0.0000 2.477e+09
  [7,]  0 0.0000 1.874e+09
  [8,]  0 0.0000 1.417e+09
  [9,]  0 0.0000 1.072e+09
 [10,]  0 0.0000 8.111e+08
 [11,]  0 0.0000 6.136e+08
 [12,]  0 0.0000 4.642e+08
 [13,]  0 0.0000 3.511e+08
 [14,]  0 0.0000 2.656e+08
 [15,]  0 0.0000 2.009e+08
 [16,]  0 0.0000 1.520e+08
 [17,]  0 0.0000 1.150e+08
 [18,]  0 0.0000 8.697e+07
 [19,]  0 0.0000 6.579e+07
 [20,]  0 0.0000 4.977e+07
 [21,]  0 0.0000 3.765e+07
 [22,]  0 0.0000 2.848e+07
 [23,]  0 0.0000 2.154e+07
 [24,]  0 0.0000 1.630e+07
 [25,]  0 0.0000 1.233e+07
 [26,]  0 0.0000 9.326e+06
 [27,]  0 0.0000 7.055e+06
 [28,]  0 0.0000 5.337e+06
 [29,]  0 0.0000 4.037e+06
 [30,]  0 0.0000 3.054e+06
 [31,]  0 0.0000 2.310e+06
 [32,]  0 0.0000 1.748e+06
 [33,]  0 0.0000 1.322e+06
 [34,]  0 0.0000 1.000e+06
 [35,]  0 0.0000 7.565e+05
 [36,]  0 0.0000 5.722e+05
 [37,]  0 0.0000 4.329e+05
 [38,]  0 0.0000 3.275e+05
 [39,]  0 0.0000 2.477e+05
 [40,]  0 0.0000 1.874e+05
 [41,]  0 0.0000 1.417e+05
 [42,]  0 0.0000 1.072e+05
 [43,]  0 0.0000 8.111e+04
 [44,]  0 0.0000 6.136e+04
 [45,]  0 0.0000 4.642e+04
 [46,]  0 0.0000 3.511e+04
 [47,]  0 0.0000 2.656e+04
 [48,]  0 0.0000 2.009e+04
 [49,]  0 0.0000 1.520e+04
 [50,]  0 0.0000 1.150e+04
 [51,]  0 0.0000 8.697e+03
 [52,]  0 0.0000 6.579e+03
 [53,]  0 0.0000 4.977e+03
 [54,]  0 0.0000 3.765e+03
 [55,]  0 0.0000 2.848e+03
 [56,]  0 0.0000 2.154e+03
 [57,]  0 0.0000 1.630e+03
 [58,]  0 0.0000 1.233e+03
 [59,]  0 0.0000 9.326e+02
 [60,]  0 0.0000 7.055e+02
 [61,]  0 0.0000 5.337e+02
 [62,]  0 0.0000 4.037e+02
 [63,]  0 0.0000 3.054e+02
 [64,]  0 0.0000 2.310e+02
 [65,]  0 0.0000 1.748e+02
 [66,]  0 0.0000 1.322e+02
 [67,]  0 0.0000 1.000e+02
 [68,]  0 0.0000 7.565e+01
 [69,]  0 0.0000 5.722e+01
 [70,]  0 0.0000 4.329e+01
 [71,]  0 0.0000 3.275e+01
 [72,]  0 0.0000 2.477e+01
 [73,]  0 0.0000 1.874e+01
 [74,]  0 0.0000 1.417e+01
 [75,]  0 0.0000 1.072e+01
 [76,]  0 0.0000 8.111e+00
 [77,]  0 0.0000 6.136e+00
 [78,]  0 0.0000 4.642e+00
 [79,]  0 0.0000 3.511e+00
 [80,]  0 0.0000 2.656e+00
 [81,]  0 0.0000 2.009e+00
 [82,]  0 0.0000 1.520e+00
 [83,]  0 0.0000 1.150e+00
 [84,]  0 0.0000 8.697e-01
 [85,]  2 0.2737 6.579e-01
 [86,]  2 0.4742 4.977e-01
 [87,]  2 0.5890 3.765e-01
 [88,]  2 0.6546 2.848e-01
 [89,]  3 0.7034 2.154e-01
 [90,]  3 0.7465 1.630e-01
 [91,]  4 0.7810 1.233e-01
 [92,]  4 0.8543 9.326e-02
 [93,]  4 0.8962 7.055e-02
 [94,]  4 0.9202 5.337e-02
 [95,]  5 0.9342 4.037e-02
 [96,]  6 0.9428 3.054e-02
 [97,]  6 0.9478 2.310e-02
 [98,]  8 0.9509 1.748e-02
 [99,]  8 0.9529 1.322e-02
[100,]  9 0.9541 1.000e-02

$lambda.min
[1] 0.01

$lambda.1se
[1] 0.01747528

attr(,"class")
[1] "cv.glmnet"

This is the minimum lambda that represents the "best" model
[1] 0.01

This is the MSE of the testing data set when the training model is implemented
[1] 0.05154446

These are the official coefficients calcualted using the "best" model lambda and the full datset12 x 1 sparse Matrix of class "dgCMatrix"
                            s0
(Intercept)         .         
Income             -0.55166063
Limit               0.92504680
Rating              0.36787493
Cards               0.04499772
Age                -0.01666003
Education           .         
Gender.Male         .         
StudentYes          0.26681304
MarriedYes          .         
EthnicityAsian      .         
EthnicityCaucasian  .         
