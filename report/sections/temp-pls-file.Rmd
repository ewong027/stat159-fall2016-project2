---
output: pdf_document
---
```{r, echo = FALSE, include = FALSE}
scaled_credit <- read.csv("../../data/datasets/scaled-credit.csv")
load("../../data/RData-files/pls-regression.RData")

library(xtable)
library(png)
library(grid)

#Creating a matrix of the coefficients to be used in the analysis section.
coef_matrix <- as.data.frame(pls_coef_full)
```

Partial least squares regression, PLSR is an extension of principal components regression. It also works at finding the best linear combinations, but takes into account both the response and predictor variables. The idea behind this is to find a set of data $Z_i$ that would create a better fit for the data than the original data would be.
These $Z_i$ are calculated using the scaled data from `scaled-data.csv`. The process of calculating these values is iterative, so we begin with the first value$Z_1$. Mathematically, the formula is $$Z_1=\sum_{j=1}^p\phi_{j1}X_j$$ where $X_j$ is the jth explanatory variable and $\phi_{j1}$ is the coefficient when you perform simple linear regression of $Y$ onto $X_j$. This process gives more wieght to values that have more of a relationship with the response variable. 
Then to calcualte $Z_2$ you regress each of the varibles on $Z_1$. The residuals from these predictions will be the new set variables, $\tilde{X_i}$ on which we will perform the same process as for the $X_i$. So we will say $$Z_2=\sum_{j=1}^p\phi_{j2}\tilde{X_j}$$ where $\phi_{j2}$ is the coefficient from simple linear regression of $Y$ onto $\tilde{X_j}$. This process in continued M times. M is chasen in the cross validation step. Then once we have $Z_1, Z_2, \dots, Z_M$ we now fit a linear model in the same way as in PCR. 


#Analysis
The right M was chosen based off the MSE, the mean squared error. The following plot shows the relationship between MSEP (mean squared error of predictions) and the number of components, M.

```{r fig.width = 5, fig.height = 3, fig.align = "center", echo = FALSE, fig.cap = "MSEP Plot of Partial Least Squares Regression"} 
img <- readPNG("../../images/cv-pls-mse-plot.png")
grid.raster(img)
```
We use the M that gives the smallest MSEP/MSE. We received this minimization from cross validation. We got this analysis from the training data set, which helped us fit the best model. 

In order to test the model, we used the testing data set. We fit the model using the M from above and calculated the MSE. This will test how acccurate of a fit the model is. When we did this, we got an MSE of `r pls_mse`. 

Then using the full dataset, we came up the following coefficients:
```{r, results = "asis", echo = FALSE}
colnames(coef_matrix) <- 'Beta Hats'
print(xtable(coef_matrix, caption = 'Lasso Regresison Coefficients'), type = 'latex', comment = FALSE, digits = 5)
```
Although we got the M from the training dataset, we got these coefficients from the entire dataset, `scaled-credit.csv`. 

