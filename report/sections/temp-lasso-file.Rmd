---
output: pdf_document
---
```{r, echo = FALSE, include = FALSE}
scaled_credit <- read.csv("../../data/datasets/scaled-credit.csv")
load("../../data/RData-files/lasso-regression.RData")

library(xtable)
library(png)
library(grid)

#Creating a matrix of the coefficients to be used in the analysis section.
coef_matrix <- as.data.frame(as.matrix(lasso_coef_full))
```

##Lasso Regression 

#Method

Lasso regression is very similar to ridge regression when it comes to the process. Howvever, in lasso regression we minimize $$\sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}{p}\beta_jx_{ij})^2+\lambda\sum_{j=1}^{p}|\beta_j|$$. This is called a shrinkage method because we are puttting a limit to the "size" of the $\beta$ coefficents. Unlike in ridge regression, this method of minimization could result in estimations of the $\beta_j$ as exaclty zero. This means Lasso regression performs a variable selection and only fits a model to a subset of the data. 
In order to calcualte the best $\lambda$ for the minimization, we used ten-fold cross validation. Cross validation involves partitioning the data into subsets, fitting a model with one subset, and validating it on the the other. We select a $\lambda$ based off of the validation with the least amount of error. When we ran our cross validation on the `scaled-credit.csv` data set, we came up with a lambda of `r cat(lambda_min_lasso)`. 


#Analysis
The right lambda was chosen based off the MSE, the mean squared error. The following plot shows the relationship between MSE and the log of lambda.  

```{r fig.width = 5, fig.height = 3, fig.align = "center", echo = FALSE, fig.cap = "MSE Plot of Lasso Regression"} 
img <- readPNG("../../images/cv-lasso-mse-plot.png")
grid.raster(img)
```
We use the $\lambda$ that gives the smallest MSE. This is given by the left-most value on this graph. We got this analysis from the training data set, which helped up fit the best model. 

In order to test the model, we used the testing data set. We fit the model using the $\lambda$ from above and calculated the MSE. This will test how acccurate of a fit the model is. When we did this, we got an MSE of `r lasso_mse`. 

Then using the full dataset, we came up the following coefficients:
```{r, results = "asis", echo = FALSE}
colnames(coef_matrix) <- 'Beta Hats'
print(xtable(coef_matrix, caption = 'Lasso Regresison Coefficients'), type = 'latex', comment = FALSE, digits = 5)
```
Although we got the $\lambda$ from the training dataset, we got these coefficients from the entire dataset, `scaled-credit.csv`. Some of the coefficients have been set to zero by the lasso regression analysis. As mentioned earlier, lasso regression has a dimension reduction component and will only fit the data to the variables that fit the mse criteria. Our analysis shows six beta coefficients that have been set to zero. 



