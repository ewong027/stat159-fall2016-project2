#Methods

##Ordinary Least Squares Regression
We first perform multiple linear regression analysis on our `scaled_credit.csv` data. We use the linear model $$y \approx \beta_0 + \beta_1*x_1 + \beta_2*x_2 + \dots + \beta_11*x_11$$ to describe the relationship between `Balance` and financial and demographic information represented in the $x_is$. Therefore, the linear model looks more like this: $$Sales \approx \beta_0 + \beta_1*Income+ \beta_2*Limit + \dots + \beta_11*EthnicityCaucasian$$ where $\beta_0$ is the intercept term and the $\beta_is$ describe how each financial or demographic variable affects the sales. 
$$\hat{\beta} = \begin{bmatrix}
\hat{\beta_0}\\
\hat{\beta_1}\\
\hat{\beta_2}\\
\vdots\\
\hat{\beta_11}
\end{bmatrix}$$
is the least squares estimate of $\beta$ which contains the actual values of the $\beta_is$. By the Gauss-Markov Theorem they are the best linear unbiased estimators. They are estimated by minimizing the sum of the residual squared errors (RSS): $$RSS = e^2_1+e^2_2+ \cdots +e^2_n$$ where $e_i$ is equal to $y_i-\hat{y_i}$.  $\hat{y_i}$ is calculated by using the model and $\hat{\beta}$: $$\hat{y_i} = X\hat{\beta}$$ where $$X = \begin{bmatrix}
1 & x_{1,1} & x_{1,2} & \dots & x_{1,11}\\
1 & x_{2,1} & x_{2,2} & \dots & x_{2,11}\\
\vdots & \vdots & \vdots & \vdots & \vdots \\
1 & x_{400,1} & x_{400,2} & \dots & x_{400,11}\\
\end{bmatrix}$$
$\hat{y_i}$ is the predicted y value. In terms of this analysis, $\hat{y_i}$ is the amount of predicted balance based off of the all the different predictor variables. Basically, minimizing the RSS would be minimizing the error of the prediction. 

RSS can also be written as: $$RSS = \displaystyle\sum_{i=1}^{n} (y_i-\hat{\beta_0}-\hat{\beta_1}*x_{i,1}-\hat{\beta_2}*x_{i,2}-\dots -\hat{\beta_11}*x_{i,11})$$ Minimizing this value over the $\hat{\beta_i}s$ results in $$\hat{\beta} = (X^TX)^{-1}X^TY$$ where Y is a vector with all the y values. Using the `Advertising.csv` data we replace the $y_is$ with the `Balance` numbers, the $x_{i,1}$ with the `Income` numbers, the $x_{i,2}$ with the `Limit` numbers, and so on. 

##Ridge Regression
Ridge regression is a shrinkage method, which means that it will constrain/regularize the coefficient estimates. This effectively will cause the coefficient estimates to tend toward zero because it can help to reduce the variance. Ridge regression is very similar to ordinary least squares, but difference is in the way that the coefficients are estimated. In ridge regression, we are minimizing $$RSS + \lambda\sum_{j=1}^{p}\beta_j^{2}$$ whereas in ordinary least squares, we are minimizing $$RSS = \sum _{i=1}^{n}\left (y _{i}-\beta_0- \sum_{j=1}^{p}\beta_jx_{ij} \right )^{2}$$. From the two types of regression, you can see that in ridge regression, there is a term that ordinary least squares does not, $\lambda\sum_{j=1}^{p}\beta_j^{2}$, and this will shrink estimates of $\beta_j$ towards zero because $\lambda\sum_{j=1}^{p}\beta_j^{2}$ is smallest when all of the betas are close to zero. $\lambda$ when greater than equal to zero is known as the tuning parameter. The purpose of the turning parameter is to control the impact of RSS and $\lambda\sum_{j=1}^{p}\beta_j^{2}$ on the regression coefficients.When the tuning parameter is large, the ridge regression coefficients will tend towards zero and when the tuning parameter is equal to zero, ridge and least squares will give the same coefficient estimates. 

Ridge regression is useful because of the bias-variance trade-off. As $\lambda$ increases flexibility of ridge regression decreases, so the variance of a model will decrease as well. However, this will increase the bias. Ridge regression gets around this because when $\lambda$ increases, there will be a large decrease in variance, but only a slight increase in bias. So, mean squared error (MSE) will only increase slightly instead of a lot because MSE takes into account bias. This is why, we want to look at our results comparing ridge regression coefficients and ordinary least squares coefficients. 

##Lasso Regression
Lasso regression is very similar to ridge regression when it comes to the process. However, in lasso regression we minimize $$\sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2+\lambda\sum_{j=1}^{p}|\beta_j|$$. This is called a shrinkage method because we are putting a limit to the "size" of the $\beta$ coefficients. Unlike in ridge regression, this method of minimization could result in estimations of the $\beta_j$ as exactly zero. This means Lasso regression performs a variable selection and only fits a model to a subset of the data. 
In order to calculate the best $\lambda$ for the minimization, we used ten-fold cross validation. Cross validation involves partitioning the data into subsets, fitting a model with one subset, and validating it on the the other. We select a $\lambda$ based off of the validation with the least amount of error. When we ran our cross validation on the `scaled-credit.csv` data set, we came up with a lambda of `r cat(lambda_min_lasso)`. 

##Principal Components Regression
Principal components regression (PCR) is a dimension reduction method. What this means is that we are transforming the predictor variables and then fitting a least squares regression model to it. The idea behind PCR is that there are a small number of "principal components" within all of the variables that can explain the variability in the data as well as the relationship with the response variable. This model is better than using ordinary least squares because if we are able to find the variables that most closely relate to the variability and response variable, we can avoid over-fitting the model and our model will not pick up any unnecessary variability. 

Under PCR, if as the number of variables in the model increases, variance also increases, but bias decreases. PCR tends to fit the model better when the first couple of principal components capture most of the variability and explain most of the response. It is also crucial to standardize our variables because the scale of each variable may effect variance and thus affect the fit of the model produced in PCR.

##Partial Least Squares Regression
Partial least squares regression, PLSR is an extension of principal components regression. It also works at finding the best linear combinations, but takes into account both the response and predictor variables. The idea behind this is to find a set of data $Z_i$ that would create a better fit for the data than the original data would be.
These $Z_i$ are calculated using the scaled data from `scaled-data.csv`. The process of calculating these values is iterative, so we begin with the first value$Z_1$. Mathematically, the formula is $$Z_1=\sum_{j=1}^p\phi_{j1}X_j$$ where $X_j$ is the jth explanatory variable and $\phi_{j1}$ is the coefficient when you perform simple linear regression of $Y$ onto $X_j$. This process gives more weight to values that have more of a relationship with the response variable. 
Then to calculate $Z_2$ you regress each of the variables on $Z_1$. The residuals from these predictions will be the new set variables, $\tilde{X_i}$ on which we will perform the same process as for the $X_i$. So we will say $$Z_2=\sum_{j=1}^p\phi_{j2}\tilde{X_j}$$ where $\phi_{j2}$ is the coefficient from simple linear regression of $Y$ onto $\tilde{X_j}$. This process in continued M times. M is chosen in the cross validation step. Then once we have $Z_1, Z_2, \dots, Z_M$ we now fit a linear model in the same way as in PCR. 