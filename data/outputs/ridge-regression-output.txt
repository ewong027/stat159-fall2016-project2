Output of 10-fold Cross-Validation using Ridge Regression on the Train Data Set
$lambda
  [1] 1.000000e+10 7.564633e+09 5.722368e+09 4.328761e+09 3.274549e+09
  [6] 2.477076e+09 1.873817e+09 1.417474e+09 1.072267e+09 8.111308e+08
 [11] 6.135907e+08 4.641589e+08 3.511192e+08 2.656088e+08 2.009233e+08
 [16] 1.519911e+08 1.149757e+08 8.697490e+07 6.579332e+07 4.977024e+07
 [21] 3.764936e+07 2.848036e+07 2.154435e+07 1.629751e+07 1.232847e+07
 [26] 9.326033e+06 7.054802e+06 5.336699e+06 4.037017e+06 3.053856e+06
 [31] 2.310130e+06 1.747528e+06 1.321941e+06 1.000000e+06 7.564633e+05
 [36] 5.722368e+05 4.328761e+05 3.274549e+05 2.477076e+05 1.873817e+05
 [41] 1.417474e+05 1.072267e+05 8.111308e+04 6.135907e+04 4.641589e+04
 [46] 3.511192e+04 2.656088e+04 2.009233e+04 1.519911e+04 1.149757e+04
 [51] 8.697490e+03 6.579332e+03 4.977024e+03 3.764936e+03 2.848036e+03
 [56] 2.154435e+03 1.629751e+03 1.232847e+03 9.326033e+02 7.054802e+02
 [61] 5.336699e+02 4.037017e+02 3.053856e+02 2.310130e+02 1.747528e+02
 [66] 1.321941e+02 1.000000e+02 7.564633e+01 5.722368e+01 4.328761e+01
 [71] 3.274549e+01 2.477076e+01 1.873817e+01 1.417474e+01 1.072267e+01
 [76] 8.111308e+00 6.135907e+00 4.641589e+00 3.511192e+00 2.656088e+00
 [81] 2.009233e+00 1.519911e+00 1.149757e+00 8.697490e-01 6.579332e-01
 [86] 4.977024e-01 3.764936e-01 2.848036e-01 2.154435e-01 1.629751e-01
 [91] 1.232847e-01 9.326033e-02 7.054802e-02 5.336699e-02 4.037017e-02
 [96] 3.053856e-02 2.310130e-02 1.747528e-02 1.321941e-02 1.000000e-02

$cvm
  [1] 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302 0.96301302
  [7] 0.96301302 0.96301302 0.96301301 0.96301301 0.96301301 0.96301301
 [13] 0.96301301 0.96301301 0.96301300 0.96301300 0.96301299 0.96301298
 [19] 0.96301297 0.96301295 0.96301293 0.96301291 0.96301287 0.96301283
 [25] 0.96301276 0.96301268 0.96301258 0.96301243 0.96301224 0.96301200
 [31] 0.96301167 0.96301123 0.96301066 0.96300990 0.96300889 0.96300757
 [37] 0.96300581 0.96300349 0.96300042 0.96299637 0.96299101 0.96298392
 [43] 0.96297455 0.96296217 0.96294580 0.96292416 0.96289556 0.96285776
 [49] 0.96280778 0.96274173 0.96265442 0.96253904 0.96238655 0.96218504
 [55] 0.96191880 0.96156709 0.96110255 0.96048919 0.95967963 0.95861164
 [61] 0.95720560 0.95535244 0.95291485 0.94971363 0.94551830 0.94003523
 [67] 0.93289476 0.92363933 0.91171516 0.89647288 0.87718441 0.85308602
 [73] 0.82345830 0.78774912 0.74573416 0.69768638 0.64449847 0.58768519
 [79] 0.52920997 0.47115620 0.41532996 0.36297182 0.31467471 0.27054628
 [85] 0.23046256 0.19433843 0.16225910 0.13446532 0.11116995 0.09243009
 [91] 0.07800512 0.06739069 0.05990591 0.05483355 0.05151038 0.04939500
 [97] 0.04808934 0.04729268 0.04681525 0.04653334

$cvsd
  [1] 0.050113544 0.050113544 0.050113544 0.050113544 0.050113544 0.050113544
  [7] 0.050113544 0.050113544 0.050113544 0.050113544 0.050113544 0.050113544
 [13] 0.050113544 0.050113543 0.050113543 0.050113543 0.050113543 0.050113542
 [19] 0.050113542 0.050113541 0.050113540 0.050113538 0.050113536 0.050113534
 [25] 0.050113531 0.050113526 0.050113521 0.050113513 0.050113503 0.050113490
 [31] 0.050113473 0.050113450 0.050113420 0.050113380 0.050113327 0.050113258
 [37] 0.050113165 0.050113044 0.050112882 0.050112669 0.050112388 0.050112016
 [43] 0.050111523 0.050110873 0.050110013 0.050108876 0.050107374 0.050105388
 [49] 0.050102763 0.050099293 0.050094706 0.050088645 0.050080634 0.050070049
 [55] 0.050056062 0.050037585 0.050013182 0.049980959 0.049938429 0.049882321
 [61] 0.049808474 0.049711127 0.049583084 0.049414931 0.049194569 0.048906583
 [67] 0.048531580 0.048045567 0.047419546 0.046619582 0.045607764 0.044344583
 [73] 0.042793284 0.040926450 0.038734347 0.036233169 0.033469870 0.030519981
 [79] 0.027477022 0.024438034 0.021490804 0.018707734 0.016144213 0.013832644
 [85] 0.011778409 0.009964395 0.008367181 0.006975305 0.005805912 0.004878269
 [91] 0.004216366 0.003810496 0.003614550 0.003561789 0.003589403 0.003652409
 [97] 0.003725212 0.003794270 0.003853659 0.003902372

$cvup
  [1] 1.01312656 1.01312656 1.01312656 1.01312656 1.01312656 1.01312656
  [7] 1.01312656 1.01312656 1.01312656 1.01312656 1.01312656 1.01312655
 [13] 1.01312655 1.01312655 1.01312655 1.01312654 1.01312653 1.01312652
 [19] 1.01312651 1.01312650 1.01312647 1.01312645 1.01312641 1.01312636
 [25] 1.01312630 1.01312621 1.01312610 1.01312595 1.01312575 1.01312549
 [31] 1.01312514 1.01312468 1.01312408 1.01312328 1.01312222 1.01312082
 [37] 1.01311898 1.01311653 1.01311330 1.01310904 1.01310339 1.01309594
 [43] 1.01308608 1.01307304 1.01305582 1.01303304 1.01300294 1.01296314
 [49] 1.01291055 1.01284102 1.01274913 1.01262768 1.01246718 1.01225509
 [55] 1.01197487 1.01160467 1.01111574 1.01047015 1.00961806 1.00849396
 [61] 1.00701407 1.00506356 1.00249793 0.99912856 0.99471287 0.98894181
 [67] 0.98142634 0.97168490 0.95913471 0.94309247 0.92279217 0.89743060
 [73] 0.86625158 0.82867557 0.78446851 0.73391955 0.67796834 0.61820517
 [79] 0.55668699 0.49559423 0.43682076 0.38167955 0.33081892 0.28437892
 [85] 0.24224097 0.20430282 0.17062628 0.14144063 0.11697586 0.09730836
 [91] 0.08222149 0.07120118 0.06352046 0.05839534 0.05509978 0.05304741
 [97] 0.05181455 0.05108695 0.05066891 0.05043571

$cvlo
  [1] 0.91289947 0.91289947 0.91289947 0.91289947 0.91289947 0.91289947
  [7] 0.91289947 0.91289947 0.91289947 0.91289947 0.91289947 0.91289947
 [13] 0.91289947 0.91289946 0.91289946 0.91289945 0.91289945 0.91289944
 [19] 0.91289943 0.91289941 0.91289939 0.91289937 0.91289934 0.91289929
 [25] 0.91289923 0.91289916 0.91289905 0.91289892 0.91289874 0.91289851
 [31] 0.91289819 0.91289778 0.91289724 0.91289652 0.91289557 0.91289431
 [37] 0.91289264 0.91289045 0.91288754 0.91288370 0.91287862 0.91287190
 [43] 0.91286303 0.91285130 0.91283579 0.91281529 0.91278819 0.91275237
 [49] 0.91270502 0.91264244 0.91255972 0.91245039 0.91230591 0.91211500
 [55] 0.91186274 0.91152950 0.91108937 0.91050824 0.90974120 0.90872932
 [61] 0.90739713 0.90564131 0.90333177 0.90029869 0.89632374 0.89112864
 [67] 0.88436318 0.87559376 0.86429562 0.84985330 0.83157664 0.80874143
 [73] 0.78066501 0.74682267 0.70699982 0.66145322 0.61102860 0.55716521
 [79] 0.50173294 0.44671816 0.39383916 0.34426408 0.29853050 0.25671364
 [85] 0.21868415 0.18437403 0.15389192 0.12749002 0.10536404 0.08755182
 [91] 0.07378876 0.06358019 0.05629136 0.05127177 0.04792097 0.04574259
 [97] 0.04436413 0.04349841 0.04296159 0.04263097

$nzero
 s0  s1  s2  s3  s4  s5  s6  s7  s8  s9 s10 s11 s12 s13 s14 s15 s16 s17 s18 
 11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11 
s19 s20 s21 s22 s23 s24 s25 s26 s27 s28 s29 s30 s31 s32 s33 s34 s35 s36 s37 
 11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11 
s38 s39 s40 s41 s42 s43 s44 s45 s46 s47 s48 s49 s50 s51 s52 s53 s54 s55 s56 
 11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11 
s57 s58 s59 s60 s61 s62 s63 s64 s65 s66 s67 s68 s69 s70 s71 s72 s73 s74 s75 
 11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11 
s76 s77 s78 s79 s80 s81 s82 s83 s84 s85 s86 s87 s88 s89 s90 s91 s92 s93 s94 
 11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11  11 
s95 s96 s97 s98 s99 
 11  11  11  11  11 

$name
                 mse 
"Mean-Squared Error" 

$glmnet.fit

Call:  glmnet(x = ridge_matrix, y = train_set$Balance, lambda = grid,      intercept = FALSE, standardize = FALSE, alpha = 0) 

       Df      %Dev    Lambda
  [1,] 11 3.329e-10 1.000e+10
  [2,] 11 4.401e-10 7.565e+09
  [3,] 11 5.817e-10 5.722e+09
  [4,] 11 7.690e-10 4.329e+09
  [5,] 11 1.017e-09 3.275e+09
  [6,] 11 1.344e-09 2.477e+09
  [7,] 11 1.777e-09 1.874e+09
  [8,] 11 2.349e-09 1.417e+09
  [9,] 11 3.105e-09 1.072e+09
 [10,] 11 4.104e-09 8.111e+08
 [11,] 11 5.425e-09 6.136e+08
 [12,] 11 7.172e-09 4.642e+08
 [13,] 11 9.481e-09 3.511e+08
 [14,] 11 1.253e-08 2.656e+08
 [15,] 11 1.657e-08 2.009e+08
 [16,] 11 2.190e-08 1.520e+08
 [17,] 11 2.895e-08 1.150e+08
 [18,] 11 3.827e-08 8.697e+07
 [19,] 11 5.060e-08 6.579e+07
 [20,] 11 6.689e-08 4.977e+07
 [21,] 11 8.842e-08 3.765e+07
 [22,] 11 1.169e-07 2.848e+07
 [23,] 11 1.545e-07 2.154e+07
 [24,] 11 2.043e-07 1.630e+07
 [25,] 11 2.700e-07 1.233e+07
 [26,] 11 3.570e-07 9.326e+06
 [27,] 11 4.719e-07 7.055e+06
 [28,] 11 6.238e-07 5.337e+06
 [29,] 11 8.246e-07 4.037e+06
 [30,] 11 1.090e-06 3.054e+06
 [31,] 11 1.441e-06 2.310e+06
 [32,] 11 1.905e-06 1.748e+06
 [33,] 11 2.518e-06 1.322e+06
 [34,] 11 3.329e-06 1.000e+06
 [35,] 11 4.401e-06 7.565e+05
 [36,] 11 5.817e-06 5.722e+05
 [37,] 11 7.690e-06 4.329e+05
 [38,] 11 1.017e-05 3.275e+05
 [39,] 11 1.344e-05 2.477e+05
 [40,] 11 1.777e-05 1.874e+05
 [41,] 11 2.348e-05 1.417e+05
 [42,] 11 3.104e-05 1.072e+05
 [43,] 11 4.104e-05 8.111e+04
 [44,] 11 5.425e-05 6.136e+04
 [45,] 11 7.171e-05 4.642e+04
 [46,] 11 9.480e-05 3.511e+04
 [47,] 11 1.253e-04 2.656e+04
 [48,] 11 1.657e-04 2.009e+04
 [49,] 11 2.190e-04 1.520e+04
 [50,] 11 2.894e-04 1.150e+04
 [51,] 11 3.826e-04 8.697e+03
 [52,] 11 5.057e-04 6.579e+03
 [53,] 11 6.684e-04 4.977e+03
 [54,] 11 8.834e-04 3.765e+03
 [55,] 11 1.167e-03 2.848e+03
 [56,] 11 1.543e-03 2.154e+03
 [57,] 11 2.038e-03 1.630e+03
 [58,] 11 2.693e-03 1.233e+03
 [59,] 11 3.556e-03 9.326e+02
 [60,] 11 4.696e-03 7.055e+02
 [61,] 11 6.196e-03 5.337e+02
 [62,] 11 8.172e-03 4.037e+02
 [63,] 11 1.077e-02 3.054e+02
 [64,] 11 1.419e-02 2.310e+02
 [65,] 11 1.866e-02 1.748e+02
 [66,] 11 2.451e-02 1.322e+02
 [67,] 11 3.212e-02 1.000e+02
 [68,] 11 4.198e-02 7.565e+01
 [69,] 11 5.469e-02 5.722e+01
 [70,] 11 7.092e-02 4.329e+01
 [71,] 11 9.146e-02 3.275e+01
 [72,] 11 1.171e-01 2.477e+01
 [73,] 11 1.486e-01 1.874e+01
 [74,] 11 1.865e-01 1.417e+01
 [75,] 11 2.310e-01 1.072e+01
 [76,] 11 2.819e-01 8.111e+00
 [77,] 11 3.381e-01 6.136e+00
 [78,] 11 3.979e-01 4.642e+00
 [79,] 11 4.594e-01 3.511e+00
 [80,] 11 5.203e-01 2.656e+00
 [81,] 11 5.786e-01 2.009e+00
 [82,] 11 6.331e-01 1.520e+00
 [83,] 11 6.832e-01 1.150e+00
 [84,] 11 7.288e-01 8.697e-01
 [85,] 11 7.699e-01 6.579e-01
 [86,] 11 8.068e-01 4.977e-01
 [87,] 11 8.394e-01 3.765e-01
 [88,] 11 8.674e-01 2.848e-01
 [89,] 11 8.908e-01 2.154e-01
 [90,] 11 9.095e-01 1.630e-01
 [91,] 11 9.239e-01 1.233e-01
 [92,] 11 9.344e-01 9.326e-02
 [93,] 11 9.418e-01 7.055e-02
 [94,] 11 9.468e-01 5.337e-02
 [95,] 11 9.501e-01 4.037e-02
 [96,] 11 9.522e-01 3.054e-02
 [97,] 11 9.536e-01 2.310e-02
 [98,] 11 9.544e-01 1.748e-02
 [99,] 11 9.548e-01 1.322e-02
[100,] 11 9.551e-01 1.000e-02

$lambda.min
[1] 0.01

$lambda.1se
[1] 0.03053856

attr(,"class")
[1] "cv.glmnet"

Minimum Lambda that will help us find Best Model
[1] 0.01

Ridge MSE of Test Data Set
[1] 0.0525927

Official Coefficients of Full Model using Ridge Regression
12 x 1 sparse Matrix of class "dgCMatrix"
                              1
(Intercept)         .          
Income             -0.568706769
Limit               0.718657903
Rating              0.593058827
Cards               0.044252756
Age                -0.025384931
Education          -0.005879651
GenderFemale       -0.010677773
StudentYes          0.273184181
MarriedYes         -0.011027761
EthnicityAsian      0.016378729
EthnicityCaucasian  0.011011789
