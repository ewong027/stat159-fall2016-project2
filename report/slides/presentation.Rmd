---
title: "Not so Ordinary (Least Squares) Regressions"
author: Bryana Gutierrez and Erica Wong
date: November 4, 2016
output: ioslides_presentation
---
# Purpose of the Project

## Overview of Project
- Purpose: to compare different types of regression models
    - Ordinary Least Squares
    - Ridge Regression
    - Lasso Regression
    - Principal Components Regression
    - Partial Least Squares Regression
- Based on Chapter 6 *Linear Model Selection and Regularization* of the book **An Introduction to Statistical Learning** by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani

## Data Used
- Using data that originated from Credit.csv
    - Qualitative Variables: gender, student status, martial status, and ethnicity
    - Quantitative Variables: one's age, the number of credit cards one has, years of education, income measured in thousands of dollars, credit limit, credit rating, and one's average credit card debt
- In analysis, using scaled-credit.csv
    - All of the variables are the same as those in Credit.csv
    - Modified with the **qualitative** by making them dummy variables
    - Then centered the mean and standardized the data so everything is in the same units --> allows for comparison
    
# Types of Regression

## Ordinary Least Squares (OLS)
- Start off with a linear model 
    - Using the Credit data set our model will be $$y \approx \beta_0 + \beta_1*x_1 + \beta_2*x_2 + \dots + \beta_11*x_11$$ to describe the relationship between `Balance` and financial and demographic information represented in the $x_is$.
- Minimizes RSS
    - Minimizing the RSS would be minimizing the error of the prediction
- Minimizing this value over the $\hat{\beta_i}s$ results in $$\hat{\beta} = (X^TX)^{-1}X^TY$$


## Ridge Regression
- Shrinkage method
- Minimizing $$RSS + \lambda\sum_{j=1}^{p}\beta_j^{2}$$
-Tuning parameter, $\lambda$ which controls the impact of RSS and $\lambda\sum_{j=1}^{p}\beta_j^{2}$ on the regression cofficents
    - When tuning parameter is large will tend towards zero and when small will give same predictions as OLS
- As $\lambda$ increases, flexibility of ridge regression decreases and variance of a model will decrease as well --> allows for a better fit model

## Lasso Regression
- Shrinkage method
- Minimizing $$\sum_{i=1}^{n}(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2+\lambda\sum_{j=1}^{p}|\beta_j|$$
- This method of minimization could result in estimations of the $\beta_j$ as exactly zero
- Performs a variable selection and only fits a model to a subset of the data

## Principal Components Regression (PCR)
- Dimension Reduction Method
- Theory: A small number of "principal components" can explain the variability in the data and relationship with the response variable
- Tends to fit the model better when the first couple of principal components capture most of the variability

## Partial Least Squares Regression (PLSR)
- PLSR is an extension of principal components regression
- Finds the best linear combinations, but takes into account both the response and predictor variables
- Believe that a set of data $Z_i$ that would create a better fit for the data than the original data would be
- $Z_i$ are built in a iterative process
- Once we have $Z_1, Z_2, \dots, Z_M$ we now fit a linear model in the same way as in PCR

# Why Look at Different Types of Regression?

##Use other models to improve model prediction accuracy
- Ordinary least squares is most effective when the number of observations (n) is much larger than the number of variables (p) in the model due to minimizing variance
- When n >> p is **not true** variance can be large which means there is a wider range of predictions --> accuracy is lowered

##Some variables are not correlated with response variable
- Ordinary least squares will use all predictors even if not all of them are correlated with the response --> can cause analysis to be more complicated
- Other models will set the coefficent of those uncorrelated variables to zero --> will allow for easier analysis

# Results

## Findings from Ridge Regression

## Findings from Lasso Regression

## Findings from PCR

## Findings from PLSR

# Conclusion


